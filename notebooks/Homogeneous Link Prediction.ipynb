{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c9f930a",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dab1594f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abhijit\\Documents\\GitHub\\cpsc490\\.venv\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import statistics\n",
    "\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.datasets import SNAPDataset\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import GCNConv, SAGEConv, GATConv, GINConv\n",
    "from torch_geometric.utils import negative_sampling, to_networkx\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13b84553",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a66043",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c28bc0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = T.Compose(\n",
    "    [\n",
    "        T.ToDevice(device),\n",
    "        T.RemoveIsolatedNodes(),\n",
    "        T.RandomLinkSplit(\n",
    "            num_val=0.05,\n",
    "            num_test=0.1,\n",
    "            is_undirected=True,\n",
    "            add_negative_train_samples=False,\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "dataset = SNAPDataset(\n",
    "    root=\"./data/SNAPDataset\", name=\"ego-facebook\", transform=transform\n",
    ")\n",
    "\n",
    "train_data = next(iter(DataLoader([x[0] for x in dataset], batch_size=10)))\n",
    "val_data = next(iter(DataLoader([x[1] for x in dataset], batch_size=10)))\n",
    "test_data = next(iter(DataLoader([x[2] for x in dataset], batch_size=10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74e0ba2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataBatch(x=[4167, 1406], edge_index=[2, 151430], circle=[4233], circle_batch=[4233], edge_label=[75715], edge_label_index=[2, 75715], batch=[4167], ptr=[11])\n",
      "DataBatch(x=[4167, 1406], edge_index=[2, 151430], circle=[4233], circle_batch=[4233], edge_label=[8894], edge_label_index=[2, 8894], batch=[4167], ptr=[11])\n",
      "DataBatch(x=[4167, 1406], edge_index=[2, 160324], circle=[4233], circle_batch=[4233], edge_label=[17800], edge_label_index=[2, 17800], batch=[4167], ptr=[11])\n"
     ]
    }
   ],
   "source": [
    "print(train_data)\n",
    "print(val_data)\n",
    "print(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15c1c21",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e03cbce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class SimpleNet(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, out_channels)\n",
    "\n",
    "    def encode(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        return self.conv2(x, edge_index)\n",
    "\n",
    "    def decode(self, z, edge_label_index):\n",
    "        return (z[edge_label_index[0]] * z[edge_label_index[1]]).sum(dim=-1)\n",
    "    \n",
    "    def forward(self, x, edge_index, data=None):\n",
    "        z = self.encode(x, edge_index)\n",
    "        out = model.decode(z, edge_index)\n",
    "        return torch.hstack((-out, out)).T\n",
    "\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "        # TODO: look into SAGEConv, GATConv, GINConv, comparison between\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, out_channels)\n",
    "        \n",
    "        self.W1 = nn.Linear(out_channels * 2, out_channels)\n",
    "        self.W2 = nn.Linear(out_channels, 1)\n",
    "\n",
    "    def encode(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        return self.conv2(x, edge_index)\n",
    "\n",
    "    def decode(self, z, edge_label_index):\n",
    "        z1 = torch.cat((z[edge_label_index[0]], z[edge_label_index[1]]), dim=1)\n",
    "        out1 = self.W2(F.relu(self.W1(z1)).squeeze())\n",
    "        \n",
    "        z2 = torch.cat((z[edge_label_index[1]], z[edge_label_index[0]]), dim=1)\n",
    "        out2 = self.W2(F.relu(self.W1(z2)).squeeze())\n",
    "        \n",
    "        return (out1 + out2) / 2\n",
    "    \n",
    "    def forward(self, x, edge_index, edge_label_index, data=None):\n",
    "        z = self.encode(x, edge_index)\n",
    "        out = model.decode(z, edge_label_index)\n",
    "        return torch.hstack((-out, out)).T\n",
    "\n",
    "simple_model = SimpleNet(dataset.num_features, 128, 32).to(device)\n",
    "simple_optimizer = torch.optim.Adam(params=simple_model.parameters(), lr=3e-3, weight_decay=2e-3)\n",
    "    \n",
    "model = Net(dataset.num_features, 128, 32).to(device)\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=3e-3, weight_decay=2e-3)\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "# TODO: These methods simultaneously use node feature and graph structure properties.\n",
    "#       Is it possible to train models that look at each aspect separately\n",
    "#       Can look at only node features by just passing original layer to MLP\n",
    "#       Unsure if can look at only graph by passing random vector into GCNConv\n",
    "#       Should also read up on Node2Vec and other methods of generating node embeddings (talk to Rex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89d020eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, data):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    z = model.encode(data.x, data.edge_index)\n",
    "\n",
    "    # We perform a new round of negative sampling for every training epoch:\n",
    "    neg_edge_index = negative_sampling(\n",
    "        edge_index=data.edge_index, \n",
    "        num_nodes=data.num_nodes,\n",
    "        num_neg_samples=data.edge_label_index.shape[1], \n",
    "        method='sparse'\n",
    "    )\n",
    "    \n",
    "    edge_label_index = torch.cat([data.edge_label_index, neg_edge_index], dim=-1)\n",
    "    edge_label = torch.cat([data.edge_label, data.edge_label.new_zeros(neg_edge_index.size(1))], dim=0)\n",
    "\n",
    "    out = model.decode(z, edge_label_index).view(-1)\n",
    "    loss = criterion(out, edge_label)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(model, data):\n",
    "    model.eval()\n",
    "    z = model.encode(data.x, data.edge_index)\n",
    "    out = model.decode(z, data.edge_label_index).view(-1).sigmoid()\n",
    "    a, b = data.edge_label.cpu().numpy(), out.cpu().numpy()\n",
    "    c = (out > 0.5).float().cpu().numpy()\n",
    "    return roc_auc_score(a, b), accuracy_score(a, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dab16a6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 010, Loss: 0.4942, Val: 0.8451 0.5345, Test: 0.8513 0.5370\n",
      "Epoch: 020, Loss: 0.4327, Val: 0.8577 0.5291, Test: 0.8629 0.5319\n",
      "Epoch: 030, Loss: 0.4278, Val: 0.8807 0.5426, Test: 0.8848 0.5438\n",
      "Epoch: 040, Loss: 0.4221, Val: 0.8815 0.5442, Test: 0.8849 0.5476\n",
      "Epoch: 050, Loss: 0.4210, Val: 0.8808 0.5386, Test: 0.8843 0.5407\n",
      "Epoch: 060, Loss: 0.4208, Val: 0.8873 0.5421, Test: 0.8907 0.5435\n",
      "Epoch: 070, Loss: 0.4188, Val: 0.8896 0.5407, Test: 0.8930 0.5412\n",
      "Epoch: 080, Loss: 0.4174, Val: 0.8955 0.5427, Test: 0.8993 0.5434\n",
      "Epoch: 090, Loss: 0.4168, Val: 0.8999 0.5543, Test: 0.9037 0.5535\n",
      "Epoch: 100, Loss: 0.4165, Val: 0.9014 0.5641, Test: 0.9055 0.5644\n",
      "Epoch: 110, Loss: 0.4180, Val: 0.9041 0.5625, Test: 0.9083 0.5620\n",
      "Epoch: 120, Loss: 0.4176, Val: 0.9026 0.5625, Test: 0.9068 0.5620\n",
      "Epoch: 130, Loss: 0.4185, Val: 0.9028 0.5609, Test: 0.9068 0.5596\n",
      "Epoch: 140, Loss: 0.4164, Val: 0.9046 0.5625, Test: 0.9088 0.5636\n",
      "Epoch: 150, Loss: 0.4171, Val: 0.9031 0.5631, Test: 0.9072 0.5638\n",
      "Epoch: 160, Loss: 0.4171, Val: 0.9053 0.5626, Test: 0.9096 0.5628\n",
      "Epoch: 170, Loss: 0.4160, Val: 0.9041 0.5623, Test: 0.9081 0.5624\n",
      "Epoch: 180, Loss: 0.4161, Val: 0.9046 0.5626, Test: 0.9088 0.5631\n",
      "Epoch: 190, Loss: 0.4171, Val: 0.9045 0.5627, Test: 0.9087 0.5633\n",
      "Epoch: 200, Loss: 0.4161, Val: 0.9075 0.5630, Test: 0.9117 0.5629\n",
      "Epoch: 210, Loss: 0.4164, Val: 0.9059 0.5618, Test: 0.9100 0.5619\n",
      "Epoch: 220, Loss: 0.4162, Val: 0.9039 0.5593, Test: 0.9082 0.5591\n",
      "Epoch: 230, Loss: 0.4172, Val: 0.9040 0.5593, Test: 0.9082 0.5586\n",
      "Epoch: 240, Loss: 0.4169, Val: 0.9046 0.5620, Test: 0.9088 0.5620\n",
      "Epoch: 250, Loss: 0.4159, Val: 0.9040 0.5587, Test: 0.9082 0.5587\n",
      "Epoch: 260, Loss: 0.4170, Val: 0.9061 0.5614, Test: 0.9100 0.5603\n",
      "Epoch: 270, Loss: 0.4163, Val: 0.9054 0.5625, Test: 0.9096 0.5619\n",
      "Epoch: 280, Loss: 0.4184, Val: 0.9038 0.5612, Test: 0.9078 0.5602\n",
      "Epoch: 290, Loss: 0.4188, Val: 0.9039 0.5613, Test: 0.9081 0.5610\n",
      "Epoch: 300, Loss: 0.4152, Val: 0.9035 0.5617, Test: 0.9077 0.5604\n",
      "Final Test: 0.9119 0.5622\n"
     ]
    }
   ],
   "source": [
    "best_val_auc = final_test_auc = final_test_acc = 0\n",
    "for epoch in range(1, 301):\n",
    "    loss = train(simple_model, simple_optimizer, train_data)\n",
    "    val_auc, val_acc = test(simple_model, val_data)\n",
    "    test_auc, test_acc = test(simple_model, test_data)\n",
    "    if val_auc > best_val_auc:\n",
    "        best_val_auc = val_auc\n",
    "        final_test_auc = test_auc\n",
    "        final_test_acc = test_acc\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Val: {val_auc:.4f} {val_acc:.4f}, Test: {test_auc:.4f} {test_acc:.4f}')\n",
    "\n",
    "print(f'Final Test: {final_test_auc:.4f} {final_test_acc:.4f}')\n",
    "\n",
    "simple_z = simple_model.encode(test_data.x, test_data.edge_index)\n",
    "simple_final_edge_index = simple_model.decode(simple_z, test_data.edge_label_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab773d26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 010, Loss: 0.6516, Val: 0.6647 0.5146, Test: 0.6696 0.5175\n",
      "Epoch: 020, Loss: 0.5874, Val: 0.6566 0.5806, Test: 0.6639 0.5908\n",
      "Epoch: 030, Loss: 0.5259, Val: 0.7309 0.6317, Test: 0.7386 0.6443\n",
      "Epoch: 040, Loss: 0.4354, Val: 0.7835 0.6977, Test: 0.7933 0.7119\n",
      "Epoch: 050, Loss: 0.3812, Val: 0.8182 0.7082, Test: 0.8237 0.7188\n",
      "Epoch: 060, Loss: 0.3397, Val: 0.8617 0.7684, Test: 0.8674 0.7763\n",
      "Epoch: 070, Loss: 0.3074, Val: 0.8780 0.7876, Test: 0.8841 0.7991\n",
      "Epoch: 080, Loss: 0.2834, Val: 0.8856 0.7958, Test: 0.8912 0.8078\n",
      "Epoch: 090, Loss: 0.2755, Val: 0.8886 0.8058, Test: 0.8938 0.8142\n",
      "Epoch: 100, Loss: 0.2710, Val: 0.8851 0.8080, Test: 0.8900 0.8121\n",
      "Epoch: 110, Loss: 0.2684, Val: 0.8835 0.8038, Test: 0.8882 0.8101\n",
      "Epoch: 120, Loss: 0.2666, Val: 0.8805 0.7959, Test: 0.8851 0.8020\n",
      "Epoch: 130, Loss: 0.2638, Val: 0.8799 0.7938, Test: 0.8847 0.8005\n",
      "Epoch: 140, Loss: 0.2644, Val: 0.8782 0.7919, Test: 0.8832 0.7990\n",
      "Epoch: 150, Loss: 0.2609, Val: 0.8760 0.7888, Test: 0.8811 0.7980\n",
      "Epoch: 160, Loss: 0.2624, Val: 0.8758 0.7863, Test: 0.8809 0.7948\n",
      "Epoch: 170, Loss: 0.2612, Val: 0.8770 0.7886, Test: 0.8823 0.7971\n",
      "Epoch: 180, Loss: 0.2585, Val: 0.8740 0.7857, Test: 0.8795 0.7933\n",
      "Epoch: 190, Loss: 0.2617, Val: 0.8760 0.7868, Test: 0.8814 0.7946\n",
      "Epoch: 200, Loss: 0.2586, Val: 0.8743 0.7890, Test: 0.8795 0.7979\n",
      "Epoch: 210, Loss: 0.2593, Val: 0.8750 0.7879, Test: 0.8804 0.7972\n",
      "Epoch: 220, Loss: 0.2581, Val: 0.8728 0.7854, Test: 0.8786 0.7929\n",
      "Epoch: 230, Loss: 0.2592, Val: 0.8714 0.7824, Test: 0.8773 0.7898\n",
      "Epoch: 240, Loss: 0.2563, Val: 0.8736 0.7859, Test: 0.8791 0.7944\n",
      "Epoch: 250, Loss: 0.2582, Val: 0.8739 0.7856, Test: 0.8796 0.7947\n",
      "Epoch: 260, Loss: 0.2571, Val: 0.8721 0.7850, Test: 0.8778 0.7937\n",
      "Epoch: 270, Loss: 0.2559, Val: 0.8739 0.7877, Test: 0.8798 0.7964\n",
      "Epoch: 280, Loss: 0.2550, Val: 0.8747 0.7890, Test: 0.8804 0.7971\n",
      "Epoch: 290, Loss: 0.2544, Val: 0.8746 0.7874, Test: 0.8803 0.7945\n",
      "Epoch: 300, Loss: 0.2569, Val: 0.8761 0.7887, Test: 0.8816 0.7987\n",
      "Final Test: 0.8943 0.8141\n"
     ]
    }
   ],
   "source": [
    "best_val_auc = final_test_auc = final_test_acc = 0\n",
    "for epoch in range(1, 301):\n",
    "    loss = train(model, optimizer, train_data)\n",
    "    val_auc, val_acc = test(model, val_data)\n",
    "    test_auc, test_acc = test(model, test_data)\n",
    "    if val_auc > best_val_auc:\n",
    "        best_val_auc = val_auc\n",
    "        final_test_auc = test_auc\n",
    "        final_test_acc = test_acc\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Val: {val_auc:.4f} {val_acc:.4f}, Test: {test_auc:.4f} {test_acc:.4f}')\n",
    "\n",
    "print(f'Final Test: {final_test_auc:.4f} {final_test_acc:.4f}')\n",
    "\n",
    "z = model.encode(test_data.x, test_data.edge_index)\n",
    "final_edge_index = model.decode(z, test_data.edge_label_index)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
