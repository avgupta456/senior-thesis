{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65fe1297",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abhijit\\Documents\\GitHub\\cpsc490\\.venv\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading https://snap.stanford.edu/data/facebook.tar.gz\n",
      "Extracting data\\SNAPDataset\\ego-facebook\\raw\\facebook.tar.gz\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.datasets import SNAPDataset\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Load the dataset\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "orig_transform = T.Compose(\n",
    "    [\n",
    "        T.ToDevice(device),\n",
    "        T.RemoveIsolatedNodes(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "transform = T.Compose(\n",
    "    [\n",
    "        T.ToDevice(device),\n",
    "        T.RemoveIsolatedNodes(),\n",
    "        T.RandomLinkSplit(\n",
    "            num_val=0.05,\n",
    "            num_test=0.1,\n",
    "            is_undirected=True,\n",
    "            add_negative_train_samples=False,\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "dataset = SNAPDataset(\n",
    "    root=\"./data/SNAPDataset\", name=\"ego-facebook\", transform=transform\n",
    ")\n",
    "train_data, val_data, test_data = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dace3de3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 050, Loss: 0.4899, Val: 0.9342 0.7887, Test: 0.9022 0.7491\n",
      "Epoch: 100, Loss: 0.4601, Val: 0.9398 0.8099, Test: 0.9011 0.7439\n",
      "Epoch: 150, Loss: 0.4547, Val: 0.9446 0.7852, Test: 0.9059 0.7439\n",
      "Epoch: 200, Loss: 0.4549, Val: 0.9267 0.8063, Test: 0.8937 0.7404\n",
      "Epoch: 250, Loss: 0.4351, Val: 0.9289 0.8169, Test: 0.8961 0.7544\n",
      "Epoch: 300, Loss: 0.4271, Val: 0.9305 0.8099, Test: 0.8965 0.7544\n",
      "Epoch: 350, Loss: 0.4293, Val: 0.9234 0.8063, Test: 0.8909 0.7491\n",
      "Epoch: 400, Loss: 0.4257, Val: 0.9271 0.7923, Test: 0.8909 0.7404\n",
      "Epoch: 450, Loss: 0.4275, Val: 0.9159 0.8063, Test: 0.8822 0.7491\n",
      "Epoch: 500, Loss: 0.4137, Val: 0.9168 0.8169, Test: 0.8813 0.7614\n",
      "Epoch: 550, Loss: 0.4199, Val: 0.9149 0.8028, Test: 0.8810 0.7491\n",
      "Epoch: 600, Loss: 0.4223, Val: 0.9051 0.8169, Test: 0.8756 0.7509\n",
      "Epoch: 650, Loss: 0.4117, Val: 0.9117 0.8099, Test: 0.8785 0.7649\n",
      "Epoch: 700, Loss: 0.4144, Val: 0.9142 0.8099, Test: 0.8825 0.7667\n",
      "Epoch: 750, Loss: 0.4168, Val: 0.9085 0.8063, Test: 0.8746 0.7667\n",
      "Epoch: 800, Loss: 0.4099, Val: 0.9098 0.8204, Test: 0.8745 0.7702\n",
      "Epoch: 850, Loss: 0.3943, Val: 0.9102 0.8239, Test: 0.8778 0.7737\n",
      "Epoch: 900, Loss: 0.4248, Val: 0.9062 0.8239, Test: 0.8679 0.7719\n",
      "Epoch: 950, Loss: 0.4047, Val: 0.9063 0.8134, Test: 0.8678 0.7649\n",
      "Epoch: 1000, Loss: 0.4022, Val: 0.9085 0.8169, Test: 0.8724 0.7737\n",
      "Final Test: 0.9136 0.7544\n",
      "\n",
      "Epoch: 050, Loss: 0.4053, Val: 0.9383 0.8838, Test: 0.9024 0.8333\n",
      "Epoch: 100, Loss: 0.3121, Val: 0.9466 0.8556, Test: 0.9364 0.8614\n",
      "Epoch: 150, Loss: 0.2881, Val: 0.9607 0.8873, Test: 0.9433 0.8772\n",
      "Epoch: 200, Loss: 0.2643, Val: 0.9633 0.8768, Test: 0.9456 0.8965\n",
      "Epoch: 250, Loss: 0.2677, Val: 0.9655 0.8873, Test: 0.9435 0.8825\n",
      "Epoch: 300, Loss: 0.2562, Val: 0.9672 0.8873, Test: 0.9426 0.8789\n",
      "Epoch: 350, Loss: 0.2695, Val: 0.9693 0.9014, Test: 0.9406 0.8772\n",
      "Epoch: 400, Loss: 0.2490, Val: 0.9699 0.9085, Test: 0.9436 0.8895\n",
      "Epoch: 450, Loss: 0.2486, Val: 0.9712 0.9049, Test: 0.9425 0.8842\n",
      "Epoch: 500, Loss: 0.2514, Val: 0.9711 0.9225, Test: 0.9436 0.8842\n",
      "Epoch: 550, Loss: 0.2497, Val: 0.9727 0.9225, Test: 0.9434 0.8789\n",
      "Epoch: 600, Loss: 0.2387, Val: 0.9726 0.9366, Test: 0.9436 0.8754\n",
      "Epoch: 650, Loss: 0.2452, Val: 0.9731 0.9120, Test: 0.9434 0.8842\n",
      "Epoch: 700, Loss: 0.2320, Val: 0.9710 0.9155, Test: 0.9438 0.8842\n",
      "Epoch: 750, Loss: 0.2255, Val: 0.9720 0.9225, Test: 0.9431 0.8754\n",
      "Epoch: 800, Loss: 0.2302, Val: 0.9710 0.9296, Test: 0.9414 0.8684\n",
      "Epoch: 850, Loss: 0.2423, Val: 0.9708 0.9085, Test: 0.9390 0.8754\n",
      "Epoch: 900, Loss: 0.2202, Val: 0.9711 0.9085, Test: 0.9422 0.8807\n",
      "Epoch: 950, Loss: 0.2245, Val: 0.9712 0.9366, Test: 0.9430 0.8702\n",
      "Epoch: 1000, Loss: 0.2241, Val: 0.9708 0.9014, Test: 0.9436 0.8860\n",
      "Final Test: 0.9431 0.8860\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from torch import nn\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.utils import negative_sampling\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "\n",
    "class SimpleNet(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, out_channels)\n",
    "\n",
    "    def encode(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        return self.conv2(x, edge_index)\n",
    "\n",
    "    def decode(self, z, edge_label_index):\n",
    "        return (z[edge_label_index[0]] * z[edge_label_index[1]]).sum(dim=-1)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_label_index, data=None):\n",
    "        z = self.encode(x, edge_index)\n",
    "        out = self.decode(z, edge_label_index)\n",
    "        return torch.hstack((-out, out)).T\n",
    "\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "        # TODO: look into SAGEConv, GATConv, GINConv, comparison between\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, out_channels)\n",
    "\n",
    "        self.W1 = nn.Linear(out_channels * 2, out_channels)\n",
    "        self.W2 = nn.Linear(out_channels, 1)\n",
    "\n",
    "    def encode(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        return self.conv2(x, edge_index)\n",
    "\n",
    "    def decode(self, z, edge_label_index):\n",
    "        z1 = torch.cat((z[edge_label_index[0]], z[edge_label_index[1]]), dim=1)\n",
    "        out1 = self.W2(F.relu(self.W1(z1)).squeeze())\n",
    "\n",
    "        z2 = torch.cat((z[edge_label_index[1]], z[edge_label_index[0]]), dim=1)\n",
    "        out2 = self.W2(F.relu(self.W1(z2)).squeeze())\n",
    "\n",
    "        return (out1 + out2) / 2\n",
    "\n",
    "    def forward(self, x, edge_index, edge_label_index, data=None):\n",
    "        z = self.encode(x, edge_index)\n",
    "        out = self.decode(z, edge_label_index)\n",
    "        return torch.hstack((-out, out)).T\n",
    "\n",
    "\n",
    "def train(model, optimizer, criterion, data):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    z = model.encode(data.x, data.edge_index)\n",
    "\n",
    "    # We perform a new round of negative sampling for every training epoch:\n",
    "    neg_edge_index = negative_sampling(\n",
    "        edge_index=data.edge_index,\n",
    "        num_nodes=data.num_nodes,\n",
    "        num_neg_samples=data.edge_label_index.shape[1],\n",
    "        method=\"sparse\",\n",
    "    )\n",
    "\n",
    "    edge_label_index = torch.cat([data.edge_label_index, neg_edge_index], dim=-1)\n",
    "    edge_label = torch.cat(\n",
    "        [data.edge_label, data.edge_label.new_zeros(neg_edge_index.size(1))], dim=0\n",
    "    )\n",
    "\n",
    "    out = model.decode(z, edge_label_index).view(-1)\n",
    "    loss = criterion(out, edge_label)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(model, data):\n",
    "    model.eval()\n",
    "    z = model.encode(data.x, data.edge_index)\n",
    "    out = model.decode(z, data.edge_label_index).view(-1).sigmoid()\n",
    "    a, b = data.edge_label.cpu().numpy(), out.cpu().numpy()\n",
    "    c = (out > 0.5).float().cpu().numpy()\n",
    "    return roc_auc_score(a, b), accuracy_score(a, c)\n",
    "\n",
    "\n",
    "def train_simple_model(epochs):\n",
    "    simple_model = SimpleNet(dataset.num_features, 128, 32).to(device)\n",
    "    simple_optimizer = torch.optim.Adam(params=simple_model.parameters(), lr=3e-3)\n",
    "    simple_criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    best_val_auc = final_test_auc = final_test_acc = 0\n",
    "    best_model_dict = simple_model.state_dict()\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        loss = train(simple_model, simple_optimizer, simple_criterion, train_data)\n",
    "        val_auc, val_acc = test(simple_model, val_data)\n",
    "        test_auc, test_acc = test(simple_model, test_data)\n",
    "        if val_auc > best_val_auc:\n",
    "            best_val_auc = val_auc\n",
    "            final_test_auc = test_auc\n",
    "            final_test_acc = test_acc\n",
    "            best_model_dict = simple_model.state_dict()\n",
    "        if epoch % 50 == 0:\n",
    "            print(\n",
    "                f\"Epoch: {epoch:03d}, Loss: {loss:.4f}, Val: {val_auc:.4f} {val_acc:.4f}, Test: {test_auc:.4f} {test_acc:.4f}\"\n",
    "            )\n",
    "\n",
    "    print(f\"Final Test: {final_test_auc:.4f} {final_test_acc:.4f}\")\n",
    "    print()\n",
    "\n",
    "    return simple_model, best_model_dict\n",
    "\n",
    "\n",
    "def train_model(epochs):\n",
    "    model = Net(dataset.num_features, 128, 32).to(device)\n",
    "    optimizer = torch.optim.Adam(params=model.parameters(), lr=3e-3)\n",
    "    criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "    best_val_auc = final_test_auc = final_test_acc = 0\n",
    "    best_model_dict = model.state_dict()\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        loss = train(model, optimizer, criterion, train_data)\n",
    "        val_auc, val_acc = test(model, val_data)\n",
    "        test_auc, test_acc = test(model, test_data)\n",
    "        if val_auc > best_val_auc:\n",
    "            best_val_auc = val_auc\n",
    "            final_test_auc = test_auc\n",
    "            final_test_acc = test_acc\n",
    "            best_model_dict = model.state_dict()\n",
    "        if epoch % 50 == 0:\n",
    "            print(\n",
    "                f\"Epoch: {epoch:03d}, Loss: {loss:.4f}, Val: {val_auc:.4f} {val_acc:.4f}, Test: {test_auc:.4f} {test_acc:.4f}\"\n",
    "            )\n",
    "\n",
    "    print(f\"Final Test: {final_test_auc:.4f} {final_test_acc:.4f}\")\n",
    "    print()\n",
    "\n",
    "    return model, best_model_dict\n",
    "\n",
    "epochs = 1000\n",
    "\n",
    "simple_model, _ = train_simple_model(epochs)\n",
    "model, _ = train_model(epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3da8c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Explainer:\n",
    "    def __init__(self, pred_model, x, edge_index):\n",
    "        self.pred_model = pred_model\n",
    "        self.x = x\n",
    "        self.edge_index = edge_index\n",
    "\n",
    "    def explain_edge(self, node_idx_1, node_idx_2):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2a001db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from numpy import ndarray\n",
    "from torch_geometric.utils import k_hop_subgraph\n",
    "\n",
    "\n",
    "def edge_centered_subgraph(node_idx_1, node_idx_2, x, edge_index, num_hops):\n",
    "    num_nodes = x.size(0)\n",
    "\n",
    "    subset_1, _, _, edge_mask_1 = k_hop_subgraph(\n",
    "        node_idx_1, num_hops, edge_index, num_nodes=num_nodes\n",
    "    )\n",
    "    subset_2, _, _, edge_mask_2 = k_hop_subgraph(\n",
    "        node_idx_2, num_hops, edge_index, num_nodes=num_nodes\n",
    "    )\n",
    "\n",
    "    # Combines two node-centered subgraphs\n",
    "    temp_node_idx = edge_index[0].new_full((num_nodes,), -1)  # full size\n",
    "    edge_mask = edge_mask_1 | edge_mask_2\n",
    "    edge_index = edge_index[:, edge_mask]  # filters out edges\n",
    "    subset = torch.cat((subset_1, subset_2)).unique()\n",
    "    temp_node_idx[subset] = torch.arange(subset.size(0), device=edge_index.device)\n",
    "    edge_index = temp_node_idx[edge_index]  # maps edge_index to [0, n]\n",
    "    x = x[subset]  # filters out nodes\n",
    "    mapping = torch.tensor(\n",
    "        [\n",
    "            (subset == node_idx_1).nonzero().item(),\n",
    "            (subset == node_idx_2).nonzero().item(),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return x, edge_index, mapping, subset, edge_mask\n",
    "\n",
    "\n",
    "def get_neighbors(edge_index, node_idx_1: int, node_idx_2: int) -> ndarray:\n",
    "    node_1_neighbors = set(\n",
    "        edge_index[:, (edge_index[0] == node_idx_1)][1].cpu().numpy()\n",
    "    )\n",
    "    node_2_neighbors = set(\n",
    "        edge_index[:, (edge_index[0] == node_idx_2)][1].cpu().numpy()\n",
    "    )\n",
    "    neighbors = np.array(list(node_1_neighbors.union(node_2_neighbors)))\n",
    "    return neighbors\n",
    "\n",
    "\n",
    "def mask_nodes(x, mask):\n",
    "    new_x = x.clone()\n",
    "    new_x[~mask] = 0\n",
    "    return new_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "326e4199",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "\n",
    "import torch\n",
    "from torch_geometric.nn import GNNExplainer as PyG_GNNExplainer\n",
    "from torch_geometric.nn.models.explainer import set_masks\n",
    "\n",
    "EPS = 1e-15\n",
    "\n",
    "\n",
    "class _GNNExplainer(PyG_GNNExplainer):\n",
    "    coeffs = {\n",
    "        \"edge_size\": 0.10,\n",
    "        \"edge_reduction\": \"sum\",\n",
    "        \"edge_ent\": 1,\n",
    "    }\n",
    "\n",
    "    def _initialize_masks(self, x, edge_index, sub_edge_mask=None):\n",
    "        (N, F), E = x.size(), edge_index.size(1)\n",
    "        self.node_feat_mask = torch.nn.Parameter(100 * torch.ones(1, F))\n",
    "\n",
    "        std = torch.nn.init.calculate_gain(\"relu\") * sqrt(2.0 / (2 * N))\n",
    "        if sub_edge_mask is None:\n",
    "            self.edge_mask = torch.nn.Parameter(torch.randn(E) * std)\n",
    "        else:\n",
    "            E_1, mask = sub_edge_mask.sum(), 100 * torch.ones(E)\n",
    "            mask[sub_edge_mask] = torch.randn(E_1) * std\n",
    "            self.edge_mask = torch.nn.Parameter(mask)\n",
    "\n",
    "    \"\"\"\n",
    "    def _loss(self, log_logits, prediction, node_idx=None):\n",
    "        error_loss = -log_logits[prediction]\n",
    "\n",
    "        m = self.edge_mask[self.sub_edge_mask].sigmoid()\n",
    "        edge_reduce = getattr(torch, self.coeffs[\"edge_reduction\"])\n",
    "        edge_size_loss = edge_reduce(m)\n",
    "        ent = -m * torch.log(m + EPS) - (1 - m) * torch.log(1 - m + EPS)\n",
    "        edge_ent_loss = ent.mean()\n",
    "\n",
    "        loss = (\n",
    "            error_loss\n",
    "            + self.coeffs[\"edge_size\"] * edge_size_loss\n",
    "            + self.coeffs[\"edge_ent\"] * edge_ent_loss\n",
    "        )\n",
    "        \n",
    "        print(\n",
    "            round(error_loss.item(), 4), \"  \\t\", \n",
    "            round(edge_size_loss.item(), 4), \"  \\t\", \n",
    "            round(edge_ent_loss.item(), 4), \"  \\t\",\n",
    "            round(loss.item(), 4), \"  \\t\"\n",
    "        )\n",
    "\n",
    "        return loss\n",
    "    \"\"\"\n",
    "    \n",
    "    def _loss(self, log_logits, prediction, node_idx = None):\n",
    "        error_loss = -log_logits[prediction].clip(-6, 6)\n",
    "                \n",
    "        m = self.edge_mask[self.sub_edge_mask].sigmoid()\n",
    "        edge_reduce = getattr(torch, self.coeffs['edge_reduction'])\n",
    "        edge_size_loss = edge_reduce(m)\n",
    "        ent = -m * torch.log(m + EPS) - (1 - m) * torch.log(1 - m + EPS)\n",
    "        edge_ent_loss = ent.mean()\n",
    "\n",
    "        loss = -error_loss * (1 - torch.mean(m)) - 1 * edge_ent_loss\n",
    "        \n",
    "        print(\n",
    "            round(error_loss.item(), 4), \"  \\t\", \n",
    "            round(edge_size_loss.item(), 4), \"  \\t\", \n",
    "            round(edge_ent_loss.item(), 4), \"  \\t\",\n",
    "            round(loss.item(), 4), \"  \\t\"\n",
    "        )\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def explain_edge(self, node_idx_1, node_idx_2, x, edge_index):\n",
    "        self.model.eval()\n",
    "        self._clear_masks()\n",
    "\n",
    "        num_edges = edge_index.size(1)\n",
    "\n",
    "        # Only operate on a k-hop subgraph around `node_idx_1` and `node_idx_2.\n",
    "        (\n",
    "            x,\n",
    "            edge_index,\n",
    "            mapping,\n",
    "            _,\n",
    "            hard_edge_mask,\n",
    "        ) = edge_centered_subgraph(node_idx_1, node_idx_2, x, edge_index, self.num_hops)\n",
    "\n",
    "        # Only optimizes the edges from neighbors to node_1/node_2, other direction not needed for prediction\n",
    "        self.sub_edge_mask = (edge_index[1] == mapping[0]) | (\n",
    "            edge_index[1] == mapping[1]\n",
    "        )\n",
    "        edge_label_index = mapping.unsqueeze(1)\n",
    "\n",
    "        # Get the initial prediction\n",
    "        prediction = self.get_initial_prediction(\n",
    "            x, edge_index, edge_label_index=edge_label_index\n",
    "        )\n",
    "\n",
    "        self._initialize_masks(x, edge_index, self.sub_edge_mask)\n",
    "        self.to(x.device)\n",
    "\n",
    "        set_masks(self.model, self.edge_mask, edge_index, apply_sigmoid=True)\n",
    "        optimizer = torch.optim.Adam([self.edge_mask], lr=self.lr)\n",
    "\n",
    "        for epoch in range(1, self.epochs + 1):\n",
    "            optimizer.zero_grad()\n",
    "            out = self.model(\n",
    "                x=x, edge_index=edge_index, edge_label_index=edge_label_index\n",
    "            )\n",
    "            loss = self.get_loss(out, prediction, mapping).mean()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        edge_mask = self.edge_mask.new_zeros(num_edges)\n",
    "        edge_mask[hard_edge_mask] = self.edge_mask.detach().sigmoid()\n",
    "\n",
    "        self._clear_masks()\n",
    "\n",
    "        return edge_mask\n",
    "\n",
    "\n",
    "class GNNExplainer(Explainer):\n",
    "    def __init__(self, pred_model, x, edge_index, epochs=200, lr=1e-2):\n",
    "        super().__init__(pred_model, x, edge_index)\n",
    "        self.explainer = _GNNExplainer(pred_model, epochs=epochs, lr=lr)\n",
    "\n",
    "    def explain_edge(self, node_idx_1, node_idx_2):\n",
    "        edge_mask = self.explainer.explain_edge(\n",
    "            node_idx_1, node_idx_2, self.x, self.edge_index\n",
    "        )\n",
    "\n",
    "        output = {}\n",
    "        edge_filter = (self.edge_index[1] == node_idx_1) | (\n",
    "            self.edge_index[1] == node_idx_2\n",
    "        )\n",
    "        temp_edge_index = self.edge_index[0, edge_filter].cpu().numpy()\n",
    "        temp_edge_mask = edge_mask[edge_filter].cpu().numpy()\n",
    "        for node_idx, weight in zip(temp_edge_index, temp_edge_mask):\n",
    "            output[node_idx] = (output.get(node_idx, weight) + weight) / 2\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c2ac4516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nodes: 24 187\n",
      "Neighorhood Size: 81\n",
      "\n",
      "0.5718   \t 51.7357   \t 0.6925   \t -0.9798   \t\n",
      "0.8111   \t 49.2917   \t 0.6911   \t -1.1177   \t\n",
      "1.0504   \t 46.8579   \t 0.6871   \t -1.2643   \t\n",
      "1.2891   \t 44.4966   \t 0.681   \t -1.4185   \t\n",
      "1.5253   \t 42.1776   \t 0.6725   \t -1.5792   \t\n",
      "1.7579   \t 39.9029   \t 0.6619   \t -1.7453   \t\n",
      "1.9857   \t 37.6796   \t 0.6492   \t -1.9154   \t\n",
      "2.2075   \t 35.5158   \t 0.6345   \t -2.0882   \t\n",
      "2.4218   \t 33.4199   \t 0.6181   \t -2.2617   \t\n",
      "2.6284   \t 31.3996   \t 0.6002   \t -2.4351   \t\n",
      "2.8251   \t 29.4617   \t 0.581   \t -2.6058   \t\n",
      "2.978   \t 27.6165   \t 0.5608   \t -2.748   \t\n",
      "3.1015   \t 25.8753   \t 0.5402   \t -2.87   \t\n",
      "3.1905   \t 24.2541   \t 0.5197   \t -2.9661   \t\n",
      "3.2551   \t 22.7587   \t 0.4997   \t -3.0425   \t\n",
      "3.3184   \t 21.398   \t 0.4807   \t -3.1163   \t\n",
      "3.3799   \t 20.1619   \t 0.4626   \t -3.1873   \t\n",
      "3.4391   \t 19.0401   \t 0.4455   \t -3.255   \t\n",
      "3.4921   \t 18.0228   \t 0.4293   \t -3.3163   \t\n",
      "3.539   \t 17.1133   \t 0.4144   \t -3.371   \t\n",
      "3.5806   \t 16.3086   \t 0.4008   \t -3.4199   \t\n",
      "3.6199   \t 15.6016   \t 0.3884   \t -3.4653   \t\n",
      "3.6574   \t 14.9852   \t 0.3773   \t -3.5077   \t\n",
      "3.6935   \t 14.4505   \t 0.3674   \t -3.5476   \t\n",
      "3.7264   \t 13.9959   \t 0.3586   \t -3.5835   \t\n",
      "3.7536   \t 13.6202   \t 0.3511   \t -3.6131   \t\n",
      "3.7812   \t 13.3318   \t 0.3448   \t -3.6413   \t\n",
      "3.8118   \t 13.1224   \t 0.3397   \t -3.6705   \t\n",
      "3.8453   \t 12.9856   \t 0.3357   \t -3.7009   \t\n",
      "3.8819   \t 12.9158   \t 0.3325   \t -3.7323   \t\n",
      "3.9165   \t 12.9089   \t 0.3302   \t -3.7606   \t\n",
      "3.9523   \t 12.9518   \t 0.3285   \t -3.7886   \t\n",
      "3.9897   \t 13.0418   \t 0.3272   \t -3.8166   \t\n",
      "4.0391   \t 13.1763   \t 0.3264   \t -3.8537   \t\n",
      "4.09   \t 13.3615   \t 0.3258   \t -3.8904   \t\n",
      "4.1322   \t 13.5821   \t 0.3253   \t -3.9178   \t\n",
      "4.1653   \t 13.8133   \t 0.3246   \t -3.9366   \t\n",
      "4.1958   \t 14.0407   \t 0.3234   \t -3.9527   \t\n",
      "4.2239   \t 14.2613   \t 0.3219   \t -3.9665   \t\n",
      "4.2503   \t 14.4595   \t 0.3199   \t -3.9793   \t\n",
      "4.2727   \t 14.6247   \t 0.3174   \t -3.9893   \t\n",
      "4.2965   \t 14.7365   \t 0.3144   \t -4.0021   \t\n",
      "4.3212   \t 14.7983   \t 0.3108   \t -4.0171   \t\n",
      "4.3469   \t 14.7971   \t 0.3067   \t -4.0351   \t\n",
      "4.3737   \t 14.7405   \t 0.3021   \t -4.0559   \t\n",
      "4.3988   \t 14.636   \t 0.2972   \t -4.0769   \t\n",
      "4.4207   \t 14.5143   \t 0.2921   \t -4.0959   \t\n",
      "4.4402   \t 14.3895   \t 0.2871   \t -4.1129   \t\n",
      "4.4578   \t 14.2756   \t 0.2822   \t -4.1281   \t\n",
      "4.4749   \t 14.1719   \t 0.2774   \t -4.1426   \t\n",
      "4.4889   \t 14.0776   \t 0.2728   \t -4.1541   \t\n",
      "4.5022   \t 14.0034   \t 0.2684   \t -4.1644   \t\n",
      "4.5137   \t 13.9713   \t 0.2644   \t -4.1717   \t\n",
      "4.5268   \t 13.9756   \t 0.2608   \t -4.1794   \t\n",
      "4.5409   \t 14.0037   \t 0.2576   \t -4.187   \t\n",
      "4.5557   \t 14.0524   \t 0.2546   \t -4.1947   \t\n",
      "4.5712   \t 14.1188   \t 0.2518   \t -4.2024   \t\n",
      "4.5863   \t 14.2006   \t 0.2493   \t -4.2093   \t\n",
      "4.601   \t 14.2855   \t 0.2469   \t -4.2159   \t\n",
      "4.6151   \t 14.3727   \t 0.2447   \t -4.2219   \t\n",
      "4.6286   \t 14.4573   \t 0.2425   \t -4.2277   \t\n",
      "4.6416   \t 14.539   \t 0.2405   \t -4.2332   \t\n",
      "4.6541   \t 14.6177   \t 0.2385   \t -4.2385   \t\n",
      "4.6661   \t 14.6933   \t 0.2367   \t -4.2435   \t\n",
      "4.6761   \t 14.7656   \t 0.2349   \t -4.247   \t\n",
      "4.6849   \t 14.8189   \t 0.233   \t -4.2504   \t\n",
      "4.6928   \t 14.8547   \t 0.2311   \t -4.2536   \t\n",
      "4.6998   \t 14.8744   \t 0.2292   \t -4.2568   \t\n",
      "4.7059   \t 14.8751   \t 0.2272   \t -4.26   \t\n",
      "4.7111   \t 14.8586   \t 0.2253   \t -4.2633   \t\n",
      "4.7157   \t 14.8267   \t 0.2233   \t -4.2667   \t\n",
      "4.7194   \t 14.7811   \t 0.2213   \t -4.27   \t\n",
      "4.7227   \t 14.7276   \t 0.2193   \t -4.2732   \t\n",
      "4.7255   \t 14.6674   \t 0.2174   \t -4.2764   \t\n",
      "4.7278   \t 14.6012   \t 0.2154   \t -4.2795   \t\n",
      "4.7294   \t 14.53   \t 0.2135   \t -4.2822   \t\n",
      "4.7307   \t 14.4696   \t 0.2118   \t -4.2843   \t\n",
      "4.7324   \t 14.4191   \t 0.2101   \t -4.2864   \t\n",
      "4.7345   \t 14.3776   \t 0.2086   \t -4.2886   \t\n",
      "4.737   \t 14.3443   \t 0.2071   \t -4.2908   \t\n",
      "4.7398   \t 14.3185   \t 0.2058   \t -4.293   \t\n",
      "4.7425   \t 14.2995   \t 0.2045   \t -4.2949   \t\n",
      "4.7461   \t 14.2971   \t 0.2033   \t -4.297   \t\n",
      "4.7495   \t 14.3095   \t 0.2023   \t -4.2983   \t\n",
      "4.7527   \t 14.32   \t 0.2012   \t -4.2995   \t\n",
      "4.7558   \t 14.3286   \t 0.2002   \t -4.3008   \t\n",
      "4.7588   \t 14.3353   \t 0.1992   \t -4.3021   \t\n",
      "4.7618   \t 14.3401   \t 0.1982   \t -4.3034   \t\n",
      "4.7646   \t 14.3432   \t 0.1972   \t -4.3047   \t\n",
      "4.7674   \t 14.3445   \t 0.1962   \t -4.3061   \t\n",
      "4.7701   \t 14.3441   \t 0.1952   \t -4.3074   \t\n",
      "4.7727   \t 14.3423   \t 0.1943   \t -4.3088   \t\n",
      "4.7753   \t 14.3391   \t 0.1933   \t -4.3102   \t\n",
      "4.7777   \t 14.3347   \t 0.1924   \t -4.3116   \t\n",
      "4.7801   \t 14.329   \t 0.1915   \t -4.313   \t\n",
      "4.7817   \t 14.3223   \t 0.1906   \t -4.3138   \t\n",
      "4.7833   \t 14.32   \t 0.1897   \t -4.3144   \t\n",
      "4.7853   \t 14.3217   \t 0.1889   \t -4.3152   \t\n",
      "4.7875   \t 14.327   \t 0.1882   \t -4.3162   \t\n",
      "4.7899   \t 14.3355   \t 0.1875   \t -4.3172   \t\n",
      "4.7926   \t 14.3469   \t 0.1868   \t -4.3182   \t\n",
      "4.7951   \t 14.3608   \t 0.1862   \t -4.3192   \t\n",
      "4.7968   \t 14.3604   \t 0.1855   \t -4.3199   \t\n",
      "4.7979   \t 14.3471   \t 0.1847   \t -4.3207   \t\n",
      "4.7985   \t 14.322   \t 0.184   \t -4.3217   \t\n",
      "4.7985   \t 14.2864   \t 0.1832   \t -4.3225   \t\n",
      "4.799   \t 14.2689   \t 0.1825   \t -4.3231   \t\n",
      "4.8006   \t 14.2678   \t 0.1819   \t -4.3239   \t\n",
      "4.8026   \t 14.2814   \t 0.1813   \t -4.3244   \t\n",
      "4.8044   \t 14.2915   \t 0.1808   \t -4.325   \t\n",
      "4.8062   \t 14.2983   \t 0.1802   \t -4.3256   \t\n",
      "4.8078   \t 14.3019   \t 0.1796   \t -4.3263   \t\n",
      "4.8094   \t 14.3027   \t 0.1791   \t -4.327   \t\n",
      "4.8103   \t 14.2898   \t 0.1785   \t -4.3278   \t\n",
      "4.81   \t 14.2646   \t 0.1778   \t -4.3281   \t\n",
      "4.8107   \t 14.2567   \t 0.1773   \t -4.3285   \t\n",
      "4.8124   \t 14.2643   \t 0.1768   \t -4.3291   \t\n",
      "4.8147   \t 14.2859   \t 0.1763   \t -4.3297   \t\n",
      "4.8172   \t 14.3089   \t 0.1759   \t -4.3304   \t\n",
      "4.8186   \t 14.3157   \t 0.1755   \t -4.3308   \t\n",
      "4.8194   \t 14.3078   \t 0.175   \t -4.3313   \t\n",
      "4.8195   \t 14.2865   \t 0.1744   \t -4.3318   \t\n",
      "4.8198   \t 14.2708   \t 0.1739   \t -4.3323   \t\n",
      "4.8204   \t 14.2602   \t 0.1734   \t -4.3329   \t\n",
      "4.8217   \t 14.2654   \t 0.173   \t -4.3334   \t\n",
      "4.8227   \t 14.2673   \t 0.1726   \t -4.3337   \t\n",
      "4.8236   \t 14.266   \t 0.1722   \t -4.3341   \t\n",
      "4.8245   \t 14.2618   \t 0.1718   \t -4.3346   \t\n",
      "4.8251   \t 14.2549   \t 0.1713   \t -4.3351   \t\n",
      "4.8265   \t 14.2637   \t 0.1709   \t -4.3355   \t\n",
      "4.8279   \t 14.2751   \t 0.1706   \t -4.3358   \t\n",
      "4.8286   \t 14.2708   \t 0.1702   \t -4.3362   \t\n",
      "4.8285   \t 14.2521   \t 0.1697   \t -4.3365   \t\n",
      "4.8286   \t 14.2388   \t 0.1693   \t -4.3368   \t\n",
      "4.8297   \t 14.2419   \t 0.169   \t -4.3373   \t\n",
      "4.8314   \t 14.2598   \t 0.1687   \t -4.3377   \t\n",
      "4.8322   \t 14.261   \t 0.1683   \t -4.3379   \t\n",
      "4.8324   \t 14.2471   \t 0.1679   \t -4.3383   \t\n",
      "4.8324   \t 14.2312   \t 0.1675   \t -4.3387   \t\n",
      "4.8321   \t 14.2136   \t 0.1672   \t -4.3388   \t\n",
      "4.8328   \t 14.2129   \t 0.1668   \t -4.3391   \t\n",
      "4.8345   \t 14.2277   \t 0.1665   \t -4.3396   \t\n",
      "4.8366   \t 14.2563   \t 0.1663   \t -4.3399   \t\n",
      "4.8377   \t 14.2667   \t 0.166   \t -4.3401   \t\n",
      "4.8382   \t 14.2607   \t 0.1657   \t -4.3404   \t\n",
      "4.838   \t 14.2399   \t 0.1653   \t -4.3409   \t\n",
      "4.8378   \t 14.2245   \t 0.165   \t -4.3411   \t\n",
      "4.8387   \t 14.2259   \t 0.1647   \t -4.3415   \t\n",
      "4.84   \t 14.2426   \t 0.1644   \t -4.3417   \t\n",
      "4.8406   \t 14.242   \t 0.1642   \t -4.3419   \t\n",
      "4.841   \t 14.2377   \t 0.1639   \t -4.3422   \t\n",
      "4.8414   \t 14.2301   \t 0.1636   \t -4.3425   \t\n",
      "4.8415   \t 14.2194   \t 0.1632   \t -4.3428   \t\n",
      "4.8424   \t 14.2253   \t 0.163   \t -4.343   \t\n",
      "4.8434   \t 14.2341   \t 0.1627   \t -4.3433   \t\n",
      "4.8445   \t 14.2454   \t 0.1625   \t -4.3434   \t\n",
      "4.8448   \t 14.2396   \t 0.1623   \t -4.3437   \t\n",
      "4.8445   \t 14.2182   \t 0.1619   \t -4.3441   \t\n",
      "4.8448   \t 14.2145   \t 0.1617   \t -4.3443   \t\n",
      "4.8449   \t 14.2073   \t 0.1614   \t -4.3445   \t\n",
      "4.846   \t 14.2165   \t 0.1612   \t -4.3447   \t\n",
      "4.8468   \t 14.2208   \t 0.161   \t -4.345   \t\n",
      "4.8472   \t 14.2207   \t 0.1607   \t -4.3452   \t\n",
      "4.8479   \t 14.224   \t 0.1605   \t -4.3454   \t\n",
      "4.8487   \t 14.2304   \t 0.1603   \t -4.3455   \t\n",
      "4.8488   \t 14.2197   \t 0.16   \t -4.3458   \t\n",
      "4.8477   \t 14.1935   \t 0.1597   \t -4.3458   \t\n",
      "4.8477   \t 14.1858   \t 0.1595   \t -4.346   \t\n",
      "4.8489   \t 14.1948   \t 0.1593   \t -4.3464   \t\n",
      "4.8504   \t 14.2189   \t 0.1592   \t -4.3464   \t\n",
      "4.8516   \t 14.2364   \t 0.159   \t -4.3464   \t\n",
      "4.852   \t 14.2353   \t 0.1588   \t -4.3466   \t\n",
      "4.8518   \t 14.2173   \t 0.1585   \t -4.347   \t\n",
      "4.8514   \t 14.1968   \t 0.1583   \t -4.3474   \t\n",
      "4.8516   \t 14.1943   \t 0.1581   \t -4.3475   \t\n",
      "4.8529   \t 14.2082   \t 0.1579   \t -4.3478   \t\n",
      "4.8537   \t 14.2163   \t 0.1577   \t -4.3479   \t\n",
      "4.8535   \t 14.2066   \t 0.1575   \t -4.348   \t\n",
      "4.8536   \t 14.2012   \t 0.1573   \t -4.3482   \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.8541   \t 14.1997   \t 0.1571   \t -4.3484   \t\n",
      "4.8551   \t 14.2145   \t 0.1569   \t -4.3485   \t\n",
      "4.8553   \t 14.2106   \t 0.1568   \t -4.3486   \t\n",
      "4.8553   \t 14.2025   \t 0.1566   \t -4.3488   \t\n",
      "4.8552   \t 14.1907   \t 0.1563   \t -4.3491   \t\n",
      "4.856   \t 14.1963   \t 0.1562   \t -4.3493   \t\n",
      "4.8563   \t 14.1969   \t 0.156   \t -4.3494   \t\n",
      "4.8575   \t 14.2137   \t 0.1559   \t -4.3495   \t\n",
      "4.8577   \t 14.2112   \t 0.1557   \t -4.3496   \t\n",
      "4.8582   \t 14.2123   \t 0.1555   \t -4.3498   \t\n",
      "4.8587   \t 14.2165   \t 0.1554   \t -4.3499   \t\n",
      "4.8584   \t 14.2025   \t 0.1552   \t -4.3501   \t\n",
      "4.8571   \t 14.1722   \t 0.1549   \t -4.3501   \t\n",
      "4.8568   \t 14.1613   \t 0.1547   \t -4.3502   \t\n",
      "4.8576   \t 14.1682   \t 0.1546   \t -4.3505   \t\n",
      "4.858   \t 14.1698   \t 0.1545   \t -4.3506   \t\n",
      "4.8582   \t 14.1667   \t 0.1543   \t -4.3507   \t\n",
      "4.8593   \t 14.1805   \t 0.1542   \t -4.3509   \t\n",
      "4.8601   \t 14.1883   \t 0.154   \t -4.3511   \t\n",
      "4.8606   \t 14.1905   \t 0.1539   \t -4.3512   \t\n",
      "4.8618   \t 14.2091   \t 0.1538   \t -4.3513   \t\n",
      "GNNExplainer Output\n",
      "346 \t 0.9928\n",
      "341 \t 0.9792\n",
      "12 \t 0.9423\n",
      "251 \t 0.8992\n",
      "75 \t 0.8185\n",
      "164 \t 0.6229\n",
      "333 \t 0.5453\n",
      "238 \t 0.4822\n",
      "55 \t 0.4606\n",
      "270 \t 0.4164\n",
      "269 \t 0.416\n",
      "207 \t 0.4143\n",
      "222 \t 0.3897\n",
      "112 \t 0.3066\n",
      "25 \t 0.2785\n",
      "169 \t 0.2767\n",
      "103 \t 0.244\n",
      "185 \t 0.1839\n",
      "335 \t 0.1688\n",
      "8 \t 0.1178\n",
      "66 \t 0.1027\n",
      "321 \t 0.0886\n",
      "199 \t 0.0841\n",
      "290 \t 0.0637\n",
      "284 \t 0.0591\n",
      "141 \t 0.0493\n",
      "276 \t 0.0389\n",
      "303 \t 0.0367\n",
      "271 \t 0.035\n",
      "74 \t 0.0339\n",
      "175 \t 0.0332\n",
      "84 \t 0.0282\n",
      "2 \t 0.0273\n",
      "184 \t 0.0236\n",
      "273 \t 0.0233\n",
      "257 \t 0.0201\n",
      "314 \t 0.0177\n",
      "78 \t 0.0173\n",
      "289 \t 0.0163\n",
      "296 \t 0.0155\n",
      "132 \t 0.0149\n",
      "121 \t 0.0129\n",
      "279 \t 0.0129\n",
      "20 \t 0.0113\n",
      "322 \t 0.0111\n",
      "155 \t 0.0111\n",
      "118 \t 0.0108\n",
      "93 \t 0.0098\n",
      "29 \t 0.0097\n",
      "245 \t 0.0063\n",
      "72 \t 0.0053\n",
      "129 \t 0.0049\n",
      "82 \t 0.0045\n",
      "87 \t 0.004\n",
      "104 \t 0.0039\n",
      "50 \t 0.0038\n",
      "223 \t 0.0036\n",
      "220 \t 0.0034\n",
      "38 \t 0.0033\n",
      "236 \t 0.0032\n",
      "256 \t 0.0032\n",
      "147 \t 0.0032\n",
      "30 \t 0.0031\n",
      "230 \t 0.0031\n",
      "39 \t 0.0031\n",
      "97 \t 0.003\n",
      "168 \t 0.003\n",
      "102 \t 0.003\n",
      "275 \t 0.003\n",
      "331 \t 0.003\n",
      "68 \t 0.003\n",
      "157 \t 0.0029\n",
      "231 \t 0.0029\n",
      "330 \t 0.0028\n",
      "83 \t 0.0028\n",
      "324 \t 0.0027\n",
      "71 \t 0.0027\n",
      "235 \t 0.0026\n",
      "198 \t 0.0025\n",
      "64 \t 0.0025\n",
      "282 \t 0.0023\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def sample_gnnexplainer(model, x, edge_index, node_idx_1, node_idx_2):\n",
    "    # GNNExplainer, 200 queries per explanation\n",
    "    gnnexplainer = GNNExplainer(model, x, edge_index, epochs=200, lr=0.1)\n",
    "    output = gnnexplainer.explain_edge(node_idx_1, node_idx_2)\n",
    "\n",
    "    print(\"GNNExplainer Output\")\n",
    "    for node_idx, weight in sorted(output.items(), key=lambda x: -x[1]):\n",
    "        print(node_idx, \"\\t\", round(weight, 4))\n",
    "    print()\n",
    "\n",
    "    return output\n",
    "\n",
    "x, edge_index = test_data.x, test_data.edge_index \n",
    "node_idx_1, node_idx_2 = 24, 187\n",
    "\n",
    "print(\"Nodes:\", node_idx_1, node_idx_2)\n",
    "print(\n",
    "    \"Neighorhood Size:\", get_neighbors(edge_index, node_idx_1, node_idx_2).shape[0]\n",
    ")\n",
    "print()\n",
    "\n",
    "\n",
    "sample_gnnexplainer(model, x, edge_index, node_idx_1, node_idx_2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5bda11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
