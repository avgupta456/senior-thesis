{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c9f930a",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dab1594f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abhijit\\Documents\\GitHub\\cpsc490\\.venv\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import statistics\n",
    "\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.datasets import SNAPDataset, DBLP, IMDB\n",
    "from torch_geometric.nn import GCNConv, SAGEConv, GATConv, GINConv, to_hetero\n",
    "from torch_geometric.utils import negative_sampling, to_networkx\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a66043",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9d6bbbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HeteroData(\n",
       "  \u001b[1mauthor\u001b[0m={\n",
       "    x=[4057, 334],\n",
       "    y=[4057],\n",
       "    train_mask=[4057],\n",
       "    val_mask=[4057],\n",
       "    test_mask=[4057]\n",
       "  },\n",
       "  \u001b[1mpaper\u001b[0m={ x=[14328, 4231] },\n",
       "  \u001b[1mterm\u001b[0m={ x=[7723, 50] },\n",
       "  \u001b[1mconference\u001b[0m={ num_nodes=20 },\n",
       "  \u001b[1m(author, to, paper)\u001b[0m={ edge_index=[2, 19645] },\n",
       "  \u001b[1m(paper, to, author)\u001b[0m={ edge_index=[2, 19645] },\n",
       "  \u001b[1m(paper, to, term)\u001b[0m={ edge_index=[2, 85810] },\n",
       "  \u001b[1m(paper, to, conference)\u001b[0m={ edge_index=[2, 14328] },\n",
       "  \u001b[1m(term, to, paper)\u001b[0m={ edge_index=[2, 85810] },\n",
       "  \u001b[1m(conference, to, paper)\u001b[0m={ edge_index=[2, 14328] }\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DBLP(root=\"../data/DBLP\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c28bc0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "transform = T.Compose([\n",
    "    T.ToDevice(device),\n",
    "    T.RemoveIsolatedNodes(),\n",
    "    T.RandomLinkSplit(\n",
    "        num_val=0.05, \n",
    "        num_test=0.1, \n",
    "        is_undirected=True, \n",
    "        add_negative_train_samples=False,\n",
    "        edge_types=[(\"paper\", \"to\", \"author\"), (\"paper\", \"to\", \"conference\")]\n",
    "    ),\n",
    "    T.ToUndirected(),\n",
    "])\n",
    "\n",
    "dataset = DBLP(root=\"../data/DBLP\", transform=transform)\n",
    "\n",
    "train_data, val_data, test_data = dataset[0]\n",
    "\n",
    "for data in train_data, val_data, test_data:\n",
    "    del data[\"term\"]\n",
    "    del data[(\"paper\", \"to\", \"term\")]\n",
    "    del data[(\"term\", \"to\", \"paper\")]\n",
    "    del data[(\"author\", \"to\", \"paper\")]\n",
    "    del data[(\"conference\", \"to\", \"paper\")]\n",
    "    \n",
    "    del data[(\"paper\", \"rev_to\", \"author\")]\n",
    "    del data[(\"term\", \"rev_to\", \"paper\")]\n",
    "    del data[(\"paper\", \"rev_to\", \"term\")]\n",
    "    del data[(\"paper\", \"rev_to\", \"conference\")]\n",
    "    \n",
    "    data[\"conference\"].x = torch.ones((20, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "74e0ba2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HeteroData(\n",
      "  \u001b[1mauthor\u001b[0m={\n",
      "    x=[4057, 334],\n",
      "    y=[4057],\n",
      "    train_mask=[4057],\n",
      "    val_mask=[4057],\n",
      "    test_mask=[4057]\n",
      "  },\n",
      "  \u001b[1mpaper\u001b[0m={ x=[14328, 4231] },\n",
      "  \u001b[1mconference\u001b[0m={\n",
      "    num_nodes=20,\n",
      "    x=[20, 1]\n",
      "  },\n",
      "  \u001b[1m(paper, to, author)\u001b[0m={\n",
      "    edge_index=[2, 16699],\n",
      "    edge_label=[16699],\n",
      "    edge_label_index=[2, 16699]\n",
      "  },\n",
      "  \u001b[1m(paper, to, conference)\u001b[0m={\n",
      "    edge_index=[2, 12180],\n",
      "    edge_label=[12180],\n",
      "    edge_label_index=[2, 12180]\n",
      "  },\n",
      "  \u001b[1m(author, rev_to, paper)\u001b[0m={\n",
      "    edge_index=[2, 16699],\n",
      "    edge_label=[16699]\n",
      "  },\n",
      "  \u001b[1m(conference, rev_to, paper)\u001b[0m={\n",
      "    edge_index=[2, 12180],\n",
      "    edge_label=[12180]\n",
      "  }\n",
      ")\n",
      "HeteroData(\n",
      "  \u001b[1mauthor\u001b[0m={\n",
      "    x=[4057, 334],\n",
      "    y=[4057],\n",
      "    train_mask=[4057],\n",
      "    val_mask=[4057],\n",
      "    test_mask=[4057]\n",
      "  },\n",
      "  \u001b[1mpaper\u001b[0m={ x=[14328, 4231] },\n",
      "  \u001b[1mconference\u001b[0m={\n",
      "    num_nodes=20,\n",
      "    x=[20, 1]\n",
      "  },\n",
      "  \u001b[1m(paper, to, author)\u001b[0m={\n",
      "    edge_index=[2, 16699],\n",
      "    edge_label=[1964],\n",
      "    edge_label_index=[2, 1964]\n",
      "  },\n",
      "  \u001b[1m(paper, to, conference)\u001b[0m={\n",
      "    edge_index=[2, 12180],\n",
      "    edge_label=[1432],\n",
      "    edge_label_index=[2, 1432]\n",
      "  },\n",
      "  \u001b[1m(author, rev_to, paper)\u001b[0m={ edge_index=[2, 16699] },\n",
      "  \u001b[1m(conference, rev_to, paper)\u001b[0m={ edge_index=[2, 12180] }\n",
      ")\n",
      "HeteroData(\n",
      "  \u001b[1mauthor\u001b[0m={\n",
      "    x=[4057, 334],\n",
      "    y=[4057],\n",
      "    train_mask=[4057],\n",
      "    val_mask=[4057],\n",
      "    test_mask=[4057]\n",
      "  },\n",
      "  \u001b[1mpaper\u001b[0m={ x=[14328, 4231] },\n",
      "  \u001b[1mconference\u001b[0m={\n",
      "    num_nodes=20,\n",
      "    x=[20, 1]\n",
      "  },\n",
      "  \u001b[1m(paper, to, author)\u001b[0m={\n",
      "    edge_index=[2, 17681],\n",
      "    edge_label=[3928],\n",
      "    edge_label_index=[2, 3928]\n",
      "  },\n",
      "  \u001b[1m(paper, to, conference)\u001b[0m={\n",
      "    edge_index=[2, 12896],\n",
      "    edge_label=[2864],\n",
      "    edge_label_index=[2, 2864]\n",
      "  },\n",
      "  \u001b[1m(author, rev_to, paper)\u001b[0m={ edge_index=[2, 17681] },\n",
      "  \u001b[1m(conference, rev_to, paper)\u001b[0m={ edge_index=[2, 12896] }\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(train_data)\n",
    "print(val_data)\n",
    "print(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15c1c21",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e03cbce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = SAGEConv((-1, -1), hidden_channels)\n",
    "        self.conv2 = SAGEConv((-1, -1), out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        return self.conv2(x, edge_index)\n",
    "\n",
    "    \n",
    "class SimpleNet(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, out_channels, metadata):\n",
    "        super().__init__()\n",
    "        self.encoder = to_hetero(Encoder(hidden_channels=hidden_channels, out_channels=out_channels), metadata)\n",
    "    \n",
    "    def encode(self, x_dict, edge_index_dict):\n",
    "        return self.encoder(x_dict, edge_index_dict)\n",
    "    \n",
    "    def decode(self, z1, z2, edge_label_index):\n",
    "        x1 = z1[edge_label_index[0]]\n",
    "        x2 = z2[edge_label_index[1]]\n",
    "        return (x1 * x2).sum(dim=-1)\n",
    "    \n",
    "    \n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, out_channels, metadata):\n",
    "        super().__init__()\n",
    "        self.encoder = to_hetero(Encoder(hidden_channels=hidden_channels, out_channels=out_channels), metadata)\n",
    "        \n",
    "        self.W1 = nn.Linear(out_channels * 2, out_channels)\n",
    "        self.W2 = nn.Linear(out_channels, 1)\n",
    "        \n",
    "    def encode(self, x_dict, edge_index_dict):\n",
    "        return self.encoder(x_dict, edge_index_dict)\n",
    "    \n",
    "    def decode(self, z1, z2, edge_label_index):\n",
    "        z_forward = torch.cat((z1[edge_label_index[0]], z2[edge_label_index[1]]), dim=1)\n",
    "        out1 = self.W2(F.relu(self.W1(z_forward)).squeeze()).squeeze()\n",
    "        \n",
    "        z_reverse = torch.cat((z2[edge_label_index[1]], z1[edge_label_index[0]]), dim=1)\n",
    "        out2 = self.W2(F.relu(self.W1(z_reverse)).squeeze()).squeeze()\n",
    "        \n",
    "        return (out1 + out2) / 2\n",
    "    \n",
    "    \n",
    "simple_model = SimpleNet(hidden_channels=128, out_channels=32, metadata=train_data.metadata()).to(device)\n",
    "simple_optimizer = torch.optim.Adam(params=simple_model.parameters(), lr=1e-3, weight_decay=1e-1)\n",
    "\n",
    "model = Net(hidden_channels=128, out_channels=32, metadata=train_data.metadata()).to(device)\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=1e-3, weight_decay=1e-2)\n",
    "\n",
    "criterion = torch.nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "89d020eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, data, key):\n",
    "    start, _, end = key\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    z = model.encode(data.x_dict, data.edge_index_dict)\n",
    "\n",
    "    # We perform a new round of negative sampling for every training epoch:\n",
    "    neg_edge_index = negative_sampling(\n",
    "        edge_index=data.edge_index_dict[key], \n",
    "        num_nodes=(data.x_dict[start].shape[0], data.x_dict[end].shape[0]),\n",
    "        num_neg_samples=data.edge_label_index_dict[key].shape[1], \n",
    "        method='sparse'\n",
    "    )\n",
    "    \n",
    "    edge_label_index = data.edge_label_index_dict[key]\n",
    "    edge_label_index = torch.cat([edge_label_index, neg_edge_index], dim=-1)\n",
    "    \n",
    "    edge_label = data.edge_label_dict[key]\n",
    "    edge_label = torch.cat([edge_label, edge_label.new_zeros(neg_edge_index.size(1))], dim=0)\n",
    "    \n",
    "    out = model.decode(z[start], z[end], edge_label_index)\n",
    "    loss = criterion(out, edge_label)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(model, data, key):\n",
    "    start, _, end = key\n",
    "    model.eval()\n",
    "    z = model.encode(data.x_dict, data.edge_index_dict)\n",
    "    out = model.decode(z[start], z[end], data.edge_label_index_dict[key]).view(-1).sigmoid()\n",
    "    a, b = data.edge_label_dict[key].cpu().numpy(), out.cpu().numpy()\n",
    "    c = (out > 0.5).float().cpu().numpy()\n",
    "        \n",
    "    return roc_auc_score(a, b), accuracy_score(a, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dab16a6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 0.7029, Val: 0.5818 0.5127, Test: 0.6133 0.5143\n",
      "Epoch: 002, Loss: 0.7652, Val: 0.6377 0.5784, Test: 0.6385 0.5703\n",
      "Epoch: 003, Loss: 0.6491, Val: 0.6450 0.5071, Test: 0.6332 0.4980\n",
      "Epoch: 004, Loss: 0.6952, Val: 0.6757 0.5193, Test: 0.6645 0.5064\n",
      "Epoch: 005, Loss: 0.6655, Val: 0.7262 0.6538, Test: 0.7285 0.6594\n",
      "Epoch: 006, Loss: 0.6263, Val: 0.7442 0.6619, Test: 0.7602 0.6660\n",
      "Epoch: 007, Loss: 0.6260, Val: 0.7482 0.6426, Test: 0.7697 0.6403\n",
      "Epoch: 008, Loss: 0.6420, Val: 0.7570 0.6502, Test: 0.7765 0.6522\n",
      "Epoch: 009, Loss: 0.6306, Val: 0.7679 0.6696, Test: 0.7810 0.6848\n",
      "Epoch: 010, Loss: 0.6123, Val: 0.7737 0.6914, Test: 0.7785 0.6894\n",
      "Epoch: 011, Loss: 0.6004, Val: 0.7732 0.6828, Test: 0.7715 0.6884\n",
      "Epoch: 012, Loss: 0.6051, Val: 0.7726 0.6894, Test: 0.7673 0.6757\n",
      "Epoch: 013, Loss: 0.6076, Val: 0.7759 0.6909, Test: 0.7717 0.6843\n",
      "Epoch: 014, Loss: 0.6047, Val: 0.7812 0.6935, Test: 0.7811 0.6955\n",
      "Epoch: 015, Loss: 0.5955, Val: 0.7859 0.6955, Test: 0.7898 0.6920\n",
      "Epoch: 016, Loss: 0.5922, Val: 0.7876 0.6828, Test: 0.7954 0.6838\n",
      "Epoch: 017, Loss: 0.5917, Val: 0.7889 0.6858, Test: 0.7980 0.6808\n",
      "Epoch: 018, Loss: 0.5947, Val: 0.7903 0.6864, Test: 0.7987 0.6805\n",
      "Epoch: 019, Loss: 0.5931, Val: 0.7916 0.6858, Test: 0.7976 0.6843\n",
      "Epoch: 020, Loss: 0.5890, Val: 0.7916 0.6940, Test: 0.7942 0.6930\n",
      "Epoch: 021, Loss: 0.5831, Val: 0.7901 0.6970, Test: 0.7895 0.6973\n",
      "Epoch: 022, Loss: 0.5845, Val: 0.7889 0.7032, Test: 0.7859 0.6953\n",
      "Epoch: 023, Loss: 0.5848, Val: 0.7894 0.7062, Test: 0.7862 0.6983\n",
      "Epoch: 024, Loss: 0.5817, Val: 0.7917 0.6991, Test: 0.7898 0.6945\n",
      "Epoch: 025, Loss: 0.5798, Val: 0.7938 0.6965, Test: 0.7942 0.6909\n",
      "Epoch: 026, Loss: 0.5815, Val: 0.7954 0.6940, Test: 0.7977 0.6871\n",
      "Epoch: 027, Loss: 0.5808, Val: 0.7963 0.6914, Test: 0.7994 0.6851\n",
      "Epoch: 028, Loss: 0.5828, Val: 0.7967 0.6914, Test: 0.7994 0.6869\n",
      "Epoch: 029, Loss: 0.5819, Val: 0.7966 0.6965, Test: 0.7978 0.6922\n",
      "Epoch: 030, Loss: 0.5758, Val: 0.7960 0.7006, Test: 0.7951 0.6958\n",
      "Epoch: 031, Loss: 0.5778, Val: 0.7949 0.7016, Test: 0.7921 0.6948\n",
      "Epoch: 032, Loss: 0.5767, Val: 0.7944 0.7072, Test: 0.7908 0.6965\n",
      "Epoch: 033, Loss: 0.5777, Val: 0.7953 0.7062, Test: 0.7921 0.6958\n",
      "Epoch: 034, Loss: 0.5745, Val: 0.7969 0.7037, Test: 0.7951 0.6930\n",
      "Epoch: 035, Loss: 0.5752, Val: 0.7982 0.6991, Test: 0.7979 0.6927\n",
      "Epoch: 036, Loss: 0.5726, Val: 0.7990 0.6935, Test: 0.7995 0.6925\n",
      "Epoch: 037, Loss: 0.5756, Val: 0.7993 0.6925, Test: 0.7998 0.6942\n",
      "Epoch: 038, Loss: 0.5722, Val: 0.7991 0.7021, Test: 0.7987 0.6930\n",
      "Epoch: 039, Loss: 0.5726, Val: 0.7986 0.7037, Test: 0.7965 0.6935\n",
      "Epoch: 040, Loss: 0.5698, Val: 0.7980 0.7067, Test: 0.7945 0.6950\n",
      "Epoch: 041, Loss: 0.5725, Val: 0.7981 0.7082, Test: 0.7940 0.6950\n",
      "Epoch: 042, Loss: 0.5705, Val: 0.7988 0.7077, Test: 0.7955 0.6930\n",
      "Epoch: 043, Loss: 0.5671, Val: 0.7997 0.6996, Test: 0.7977 0.6948\n",
      "Epoch: 044, Loss: 0.5672, Val: 0.8003 0.7037, Test: 0.7991 0.6930\n",
      "Epoch: 045, Loss: 0.5694, Val: 0.8005 0.7021, Test: 0.7993 0.6925\n",
      "Epoch: 046, Loss: 0.5679, Val: 0.8003 0.7016, Test: 0.7983 0.6937\n",
      "Epoch: 047, Loss: 0.5681, Val: 0.8000 0.7052, Test: 0.7966 0.6930\n",
      "Epoch: 048, Loss: 0.5677, Val: 0.7993 0.7067, Test: 0.7950 0.6927\n",
      "Epoch: 049, Loss: 0.5673, Val: 0.7991 0.7062, Test: 0.7945 0.6932\n",
      "Epoch: 050, Loss: 0.5680, Val: 0.7995 0.7047, Test: 0.7951 0.6927\n",
      "Final Test: 0.7993 0.6925\n"
     ]
    }
   ],
   "source": [
    "key = (\"paper\", \"to\", \"author\")\n",
    "start, _, end = key\n",
    "\n",
    "best_val_auc = final_test_auc = final_test_acc = 0\n",
    "for epoch in range(1, 51):\n",
    "    loss = train(simple_model, simple_optimizer, train_data, key)\n",
    "    val_auc, val_acc = test(simple_model, val_data, key)\n",
    "    test_auc, test_acc = test(simple_model, test_data, key)\n",
    "    if val_auc > best_val_auc:\n",
    "        best_val_auc = val_auc\n",
    "        final_test_auc = test_auc\n",
    "        final_test_acc = test_acc\n",
    "    if epoch % 1 == 0:\n",
    "        print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Val: {val_auc:.4f} {val_acc:.4f}, Test: {test_auc:.4f} {test_acc:.4f}')\n",
    "\n",
    "print(f'Final Test: {final_test_auc:.4f} {final_test_acc:.4f}')\n",
    "\n",
    "simple_z = simple_model.encode(test_data.x_dict, test_data.edge_index_dict)\n",
    "simple_final_edge_index = simple_model.decode(simple_z[start], simple_z[end], test_data.edge_label_index_dict[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ab773d26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 0.6978, Val: 0.5112 0.5000, Test: 0.4654 0.5000\n",
      "Epoch: 002, Loss: 0.6928, Val: 0.5783 0.5148, Test: 0.5449 0.5150\n",
      "Epoch: 003, Loss: 0.6907, Val: 0.6075 0.5677, Test: 0.5928 0.5616\n",
      "Epoch: 004, Loss: 0.6898, Val: 0.6185 0.5519, Test: 0.6101 0.5494\n",
      "Epoch: 005, Loss: 0.6887, Val: 0.6202 0.5672, Test: 0.6092 0.5636\n",
      "Epoch: 006, Loss: 0.6874, Val: 0.6188 0.5871, Test: 0.6044 0.5942\n",
      "Epoch: 007, Loss: 0.6858, Val: 0.6203 0.5876, Test: 0.6048 0.6013\n",
      "Epoch: 008, Loss: 0.6845, Val: 0.6237 0.5789, Test: 0.6072 0.5807\n",
      "Epoch: 009, Loss: 0.6832, Val: 0.6288 0.5611, Test: 0.6118 0.5435\n",
      "Epoch: 010, Loss: 0.6820, Val: 0.6352 0.5570, Test: 0.6185 0.5300\n",
      "Epoch: 011, Loss: 0.6804, Val: 0.6420 0.5596, Test: 0.6265 0.5384\n",
      "Epoch: 012, Loss: 0.6790, Val: 0.6488 0.5708, Test: 0.6344 0.5670\n",
      "Epoch: 013, Loss: 0.6772, Val: 0.6546 0.5937, Test: 0.6412 0.5985\n",
      "Epoch: 014, Loss: 0.6755, Val: 0.6606 0.6166, Test: 0.6475 0.6174\n",
      "Epoch: 015, Loss: 0.6732, Val: 0.6664 0.6268, Test: 0.6535 0.6293\n",
      "Epoch: 016, Loss: 0.6710, Val: 0.6725 0.6314, Test: 0.6599 0.6421\n",
      "Epoch: 017, Loss: 0.6684, Val: 0.6794 0.6431, Test: 0.6669 0.6466\n",
      "Epoch: 018, Loss: 0.6658, Val: 0.6867 0.6517, Test: 0.6740 0.6510\n",
      "Epoch: 019, Loss: 0.6623, Val: 0.6933 0.6563, Test: 0.6807 0.6555\n",
      "Epoch: 020, Loss: 0.6589, Val: 0.7000 0.6640, Test: 0.6875 0.6619\n",
      "Epoch: 021, Loss: 0.6549, Val: 0.7074 0.6701, Test: 0.6949 0.6652\n",
      "Epoch: 022, Loss: 0.6506, Val: 0.7149 0.6721, Test: 0.7027 0.6726\n",
      "Epoch: 023, Loss: 0.6463, Val: 0.7230 0.6746, Test: 0.7109 0.6741\n",
      "Epoch: 024, Loss: 0.6414, Val: 0.7317 0.6782, Test: 0.7201 0.6772\n",
      "Epoch: 025, Loss: 0.6359, Val: 0.7412 0.6818, Test: 0.7302 0.6805\n",
      "Epoch: 026, Loss: 0.6294, Val: 0.7507 0.6818, Test: 0.7400 0.6836\n",
      "Epoch: 027, Loss: 0.6244, Val: 0.7587 0.6858, Test: 0.7487 0.6864\n",
      "Epoch: 028, Loss: 0.6181, Val: 0.7656 0.6904, Test: 0.7564 0.6884\n",
      "Epoch: 029, Loss: 0.6118, Val: 0.7712 0.6909, Test: 0.7628 0.6881\n",
      "Epoch: 030, Loss: 0.6053, Val: 0.7758 0.6940, Test: 0.7677 0.6907\n",
      "Epoch: 031, Loss: 0.5985, Val: 0.7796 0.6940, Test: 0.7718 0.6963\n",
      "Epoch: 032, Loss: 0.5938, Val: 0.7829 0.6955, Test: 0.7755 0.6958\n",
      "Epoch: 033, Loss: 0.5864, Val: 0.7857 0.6991, Test: 0.7785 0.6965\n",
      "Epoch: 034, Loss: 0.5823, Val: 0.7881 0.7006, Test: 0.7809 0.6978\n",
      "Epoch: 035, Loss: 0.5764, Val: 0.7898 0.7032, Test: 0.7823 0.7016\n",
      "Epoch: 036, Loss: 0.5741, Val: 0.7906 0.7072, Test: 0.7830 0.7037\n",
      "Epoch: 037, Loss: 0.5676, Val: 0.7909 0.7088, Test: 0.7834 0.7044\n",
      "Epoch: 038, Loss: 0.5633, Val: 0.7912 0.7103, Test: 0.7836 0.7037\n",
      "Epoch: 039, Loss: 0.5655, Val: 0.7914 0.7118, Test: 0.7837 0.7019\n",
      "Epoch: 040, Loss: 0.5610, Val: 0.7917 0.7082, Test: 0.7838 0.7024\n",
      "Epoch: 041, Loss: 0.5594, Val: 0.7919 0.7067, Test: 0.7838 0.7034\n",
      "Epoch: 042, Loss: 0.5605, Val: 0.7917 0.7077, Test: 0.7835 0.7037\n",
      "Epoch: 043, Loss: 0.5573, Val: 0.7913 0.7098, Test: 0.7832 0.7057\n",
      "Epoch: 044, Loss: 0.5557, Val: 0.7909 0.7144, Test: 0.7827 0.7072\n",
      "Epoch: 045, Loss: 0.5531, Val: 0.7905 0.7144, Test: 0.7824 0.7088\n",
      "Epoch: 046, Loss: 0.5526, Val: 0.7901 0.7133, Test: 0.7821 0.7090\n",
      "Epoch: 047, Loss: 0.5534, Val: 0.7896 0.7133, Test: 0.7816 0.7088\n",
      "Epoch: 048, Loss: 0.5510, Val: 0.7888 0.7118, Test: 0.7810 0.7093\n",
      "Epoch: 049, Loss: 0.5499, Val: 0.7878 0.7098, Test: 0.7802 0.7103\n",
      "Epoch: 050, Loss: 0.5517, Val: 0.7870 0.7098, Test: 0.7796 0.7105\n",
      "Final Test: 0.7838 0.7034\n"
     ]
    }
   ],
   "source": [
    "key = (\"paper\", \"to\", \"author\")\n",
    "start, _, end = key\n",
    "\n",
    "best_val_auc = final_test_auc = final_test_acc = 0\n",
    "for epoch in range(1, 51):\n",
    "    loss = train(model, optimizer, train_data, key)\n",
    "    val_auc, val_acc = test(model, val_data, key)\n",
    "    test_auc, test_acc = test(model, test_data, key)\n",
    "    if val_auc > best_val_auc:\n",
    "        best_val_auc = val_auc\n",
    "        final_test_auc = test_auc\n",
    "        final_test_acc = test_acc\n",
    "    if epoch % 1 == 0:\n",
    "        print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Val: {val_auc:.4f} {val_acc:.4f}, Test: {test_auc:.4f} {test_acc:.4f}')\n",
    "\n",
    "print(f'Final Test: {final_test_auc:.4f} {final_test_acc:.4f}')\n",
    "\n",
    "z = model.encode(test_data.x_dict, test_data.edge_index_dict)\n",
    "final_edge_index = model.decode(z[start], z[end], test_data.edge_label_index_dict[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6bdde4",
   "metadata": {},
   "source": [
    "## SubgraphX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a60dc185",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "706acc81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([10576,  1257])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.edge_label_index_dict[(\"paper\", \"to\", \"author\")][:, 12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2402cd55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 17681])\n",
      "torch.Size([2, 12896])\n"
     ]
    }
   ],
   "source": [
    "node_1 = 12032\n",
    "node_2 = 2955\n",
    "\n",
    "paper_to_author_index = test_data.edge_index_dict[(\"paper\", \"to\", \"author\")]\n",
    "paper_to_conference_index = test_data.edge_index_dict[(\"paper\", \"to\", \"conference\")]\n",
    "\n",
    "print(paper_to_author_index.shape)\n",
    "print(paper_to_conference_index.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "03c95398",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paper coauthors {2955, 1013}\n",
      "paper conference {16}\n",
      "author papers {12032, 4480, 13507, 12862, 12031}\n"
     ]
    }
   ],
   "source": [
    "node_1_author_neighbors = set(paper_to_author_index[:, paper_to_author_index[0] == node_1][1].cpu().numpy())\n",
    "node_1_conference_neighbors = set(paper_to_conference_index[:, paper_to_conference_index[0] == node_1][1].cpu().numpy())\n",
    "\n",
    "print(\"paper coauthors\", node_1_author_neighbors)\n",
    "print(\"paper conference\", node_1_conference_neighbors)\n",
    "\n",
    "node_2_paper_neighbors = set(paper_to_author_index[:, paper_to_author_index[1] == node_2][0].cpu().numpy())\n",
    "\n",
    "print(\"author papers\", node_2_paper_neighbors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5474dde3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abhijit\\AppData\\Local\\Temp\\ipykernel_12080\\2899712078.py:8: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at  C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\TensorAdvancedIndexing.cpp:1582.)\n",
      "  S_filter[(sub_edge_mask) & (np.random.random(sub_edge_mask.shape[0]) > 0.5)] = False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12032 \t 0.37756 \t 0.33862 \t 1.11499\n",
      "4480 \t 0.00706 \t 0.01427 \t 0.49465\n",
      "13507 \t 0.04632 \t 0.01014 \t 4.56798\n",
      "12862 \t 0.40277 \t 0.32232 \t 1.24958\n",
      "12031 \t 0.20102 \t 0.30044 \t 0.66909\n"
     ]
    }
   ],
   "source": [
    "T = 5\n",
    "for neighbor in node_2_paper_neighbors:\n",
    "    pred_diffs = []\n",
    "    sub_edge_mask = paper_to_author_index[1] == node_2\n",
    "    for t in range(T):\n",
    "        S_filter = torch.zeros(paper_to_author_index.shape[1], dtype=bool)\n",
    "        S_filter[sub_edge_mask] = True\n",
    "        S_filter[(sub_edge_mask) & (np.random.random(sub_edge_mask.shape[0]) > 0.5)] = False\n",
    "        S_filter[(paper_to_author_index[0] == neighbor)] = False\n",
    "        \n",
    "        temp_edge_index_dict = {k: v for k, v in test_data.edge_index_dict.items()}\n",
    "        temp_edge_index_dict[(\"paper\", \"to\", \"author\")] = paper_to_author_index[:, S_filter]\n",
    "        \n",
    "        old_z = model.encode(test_data.x_dict, temp_edge_index_dict)\n",
    "        old_pred = model.decode(old_z[\"paper\"], old_z[\"author\"], torch.tensor([[node_1], [node_2]]))\n",
    "        \n",
    "        S_filter[(paper_to_author_index[0] == neighbor)] = True\n",
    "        temp_edge_index_dict = {k: v for k, v in test_data.edge_index_dict.items()}\n",
    "        temp_edge_index_dict[(\"paper\", \"to\", \"author\")] = paper_to_author_index[:, S_filter]\n",
    "        \n",
    "        new_z = model.encode(test_data.x_dict, temp_edge_index_dict)\n",
    "        new_pred = model.decode(new_z[\"paper\"], new_z[\"author\"], torch.tensor([[node_1], [node_2]]))\n",
    "        \n",
    "        pred_diff = (new_pred - old_pred)\n",
    "        pred_diffs.append(pred_diff.item())\n",
    "    diff_avg, diff_std = sum(pred_diffs) / len(pred_diffs), statistics.stdev(pred_diffs) / np.sqrt(T)\n",
    "    print(neighbor, \"\\t\", round(diff_avg, 5), \"\\t\", round(diff_std, 5), \"\\t\", round(diff_avg / diff_std, 5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
